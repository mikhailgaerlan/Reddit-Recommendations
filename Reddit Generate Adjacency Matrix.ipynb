{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import praw\n",
    "import pprint\n",
    "import urllib.request\n",
    "import codecs\n",
    "import re\n",
    "import multiprocessing\n",
    "import time\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from networkx.drawing.nx_agraph import graphviz_layout as layout\n",
    "\n",
    "#import joblib\n",
    "#from joblib import Parallel, delayed\n",
    "#from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = pd.read_csv('../Data/44_million_votes.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables for matrix construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each unique subreddit/user interactions.\n",
    "subreddit_user_data = reddit_data[['USERNAME','SUBREDDIT']].drop_duplicates()\n",
    "\n",
    "# Get all unique subreddits and users in a list.\n",
    "all_subreddits = list(subreddit_user_data.SUBREDDIT.unique())\n",
    "reddit_usernames = list(subreddit_user_data.USERNAME.unique())\n",
    "\n",
    "# Numerical index as a dictionary for each subreddit.\n",
    "subreddit_index = {all_subreddits[i]: i for i in range(len(all_subreddits))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside memory for adjacency matrix. Approx file size ~40 GB.\n",
    "reddit_adj_mat = np.memmap('reddit_adj_mat',dtype='int16',shape=(len(all_subreddits),len(all_subreddits)),mode='w+')\n",
    "\n",
    "# For each user add one to each subreddit they voted. Generates symmetric matrix.\n",
    "def add_to_adj(user):\n",
    "    indices = np.array(subreddit_user_data.loc[subreddit_user_data['USERNAME'].isin([user]),'SUBREDDIT'])\n",
    "    indices = [subreddit_index[sub] for sub in all_subreddits if sub in indices]\n",
    "    for i in indices:\n",
    "        reddit_adj_mat[i,indices] += 1\n",
    "\n",
    "print('Start')\n",
    "start = time.time()\n",
    "\n",
    "# Start constructing adjacency matrix. Time taken ~4.5 hours with 8 CPUs.\n",
    "p = multiprocessing.Pool(8)\n",
    "p.map(add_to_adj,reddit_usernames)\n",
    "print(time.time() - start)\n",
    "\n",
    "# Convert dense matrix to sparse matrix. Time taken ~3 min.\n",
    "reddit_adj_mat_sp = scipy.sparse.csr_matrix(reddit_adj_mat)\n",
    "print(time.time() - start)\n",
    "\n",
    "# Save sparse matrix. Approx file size ~500 MB.\n",
    "scipy.sparse.save_npz('reddit_adj_mat_sp',reddit_adj_mat_sp)\n",
    "print(time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
